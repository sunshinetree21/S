{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정보 엔트로피(Info Entropy)\n",
    "\n",
    "> 정보 엔트로피는 얼마만큼의 정보를 담고 있는지, 그리고 정보의 무질서도(Disorder)와 불확실성(Uncertainty)을 나타낸다.\n",
    "\n",
    "- 결정트리에서 엔트로피는 노드의 불순성(혹은 무작위성)을 나타내며, 0과 1 사이의 값을 갖는다.\n",
    "\n",
    "- 1로 갈수록 불순하고, 0으로 갈수록 불순하지 않다는 의미이다.</br>\n",
    "\t여기서 불순하지 않다는 것은 잘 섞이지 않았다는 의미이다. 즉 무질서도가 낮다는 것이다.\n",
    "\n",
    "- 정보 엔트로피 공식\n",
    "\n",
    "\t**정보량**\n",
    "\t$~~\\dashrightarrow~~\\boxed{-p_i\\log_2p_i}$\n",
    "\n",
    "\t**평균 정보량: 엔트로피**\n",
    "\n",
    "\t- 분할 전 $~~\\dashrightarrow ~~\\text{Entropy}=\\displaystyle\\sum_{i=1}^{n}-p_i\\log_2p_i~~$\n",
    "\t\n",
    "\t- 분할 후 $~~\\dashrightarrow ~~\\text{Entropy}=\\displaystyle\\sum_{k=1}^dR_k\\{\\sum_{i=1}^{n}-p_i\\log_2p_i\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏺ 엔트로피 설명 데이터\n",
    "\n",
    "<img src=\"https://github.com/ElaYJ/Study_Machine_Learning/assets/153154981/062360a4-8f94-41ff-bdb5-0860645524ad\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "위에서 독립변수는 키와 몸무게 2개가 있고, 라벨 값으로 보직이라는 것이 존재한다.</br>\n",
    "결정 트리는 독립변수들의 값을 어떻게 설정해야 가장 최적의 모델을 나올 것인지를 계산하게 되는데</br>\n",
    "일단 키와 몸무게의 엔트로피를 알아보도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└▶ __분류 전 엔트로피__\n",
    "\n",
    "- 위 예시 데이터를 기준으로 분류되기 전의 엔트로피 값을 계산해보면\n",
    "\n",
    "\t총 row는 8개이고 헌병 4명, 그외 4명으로 이를 엔트로피 공식에 넣을 경우 `1` 값을 얻을 수 있다.\n",
    "\n",
    "- `Entropy = 1`의 의미는 데이터가 매우 잘 섞여있다는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "(-(4/8)*np.log2(4/8)) + (-(4/8)*np.log2(4/8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└▶ __분류 후 엔트로피 - 몸무게 기준__\n",
    "\n",
    "- 몸무게가 75kg 이상일 때를 기준으로 엔트로피를 구해본다.\n",
    "\n",
    "\t- \"Yes\"의 경우,</br>\n",
    "\t표본공간 $n=3,~$ 헌병이 1명, 그외 보직이 2명 있다.\n",
    "\n",
    "\t- \"No\"의 경우,</br>\n",
    "\t표본공간 $n=5,~$ 헌병이 3명, 그외 보직이 2명 있다.</br>\n",
    "\n",
    "\t<img src=\"https://github.com/ElaYJ/Study_Machine_Learning/assets/153154981/ff8939dc-8eb1-4d1e-a0c0-7b6e44c100d4\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182958340544896"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Yes\" 조건\n",
    "\n",
    "(-(1/3)*np.log2(1/3)) + (-(2/3)*np.log2(2/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"No\" 조건\n",
    "\n",
    "(-(3/5)*np.log2(3/5)) + (-(2/5)*np.log2(2/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9512050593046015"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"몸무게 75kg 이상\" 엔트로피\n",
    "\n",
    "(3/8)*((-(1/3)*np.log2(1/3)) + (-(2/3)*np.log2(2/3))) + (5/8)*((-(3/5)*np.log2(3/5)) + (-(2/5)*np.log2(2/5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└▶ __분류 후 엔트로피 - 키 기준__\n",
    "\n",
    "- 키가 180cm 이상일 때를 기준으로 엔트로피를 구해본다.\n",
    "\n",
    "\t- \"Yes\"의 경우,</br>\n",
    "\t표본공간 $n=4,~$ 헌병이 3명, 그외 보직이 1명 있다.\n",
    "\n",
    "\t- \"No\"의 경우,</br>\n",
    "\t표본공간 $n=4,~$ 헌병이 1명, 그외 보직이 3명 있다.</br>\n",
    "\n",
    "\t<img src=\"https://github.com/ElaYJ/Study_Machine_Learning/assets/153154981/cf7217ed-6bcd-407e-bf8c-616fa9be5ff5\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Yes\" 조건\n",
    "\n",
    "(-(3/4)*np.log2(3/4)) + (-(1/4)*np.log2(1/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"No\" 조건\n",
    "\n",
    "(-(1/4)*np.log2(1/4)) + (-(3/4)*np.log2(3/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"키 180cm 이상\" 엔트로피\n",
    "\n",
    "0.5*((-(3/4)*np.log2(3/4)) + (-(1/4)*np.log2(1/4))) + 0.5*((-(1/4)*np.log2(1/4)) + (-(3/4)*np.log2(3/4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "---\n",
    "\n",
    "# 정보 이득(Info Gain)\n",
    "\n",
    "> \n",
    "\n",
    "- 엔트로피를 활용해 어떤 기준을 쓰는 것이 더 좋을지 판단하기 위해 정보 이득[Information Gain]을 이용한다.\n",
    "\n",
    "</br>\n",
    "\n",
    "- **정보이득(Information Gain) 개념**\n",
    "\n",
    "    - 정보 이득(Information Gain)은 결정 트리(Decision Tree)에서 엔트로피(Entropy)를 계산 후,</br>\n",
    "    어떤 노드를 선택하는 것이 옳은지 따져볼 때 사용하는 기댓값이다.\n",
    "\t\n",
    "    - 결정 트리에서 다양한 노드를 만들고 엔트로피를 구했다면 정보 이득이 가장 높은 값을 선택하고 다음 가지를 생성하게 되기에 엔트로피와 정보 이득은 같이 움직인다 생각하면 된다.\n",
    "\n",
    "</br>\n",
    "\n",
    "- **정보 이득 공식**\n",
    "\n",
    "    정보이득의 공식은 분할 이전 엔트로피의 값을 분할 후 엔트로피 값들로 뺀 것이다.\n",
    "    \n",
    "    $~\\text{Information Gain = (Entropy before split) - (weighted Entropy after split)}$\n",
    "    \n",
    "    - 이 둘의 차가 커질수록 잘 분리된 구조가 된다.\n",
    "    - ‘weighted Entropy’는 분할 후 최종 엔트로피 값에 분할된 노드의 비율을 곱한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⏺ 위에서 구한 엔트로피 값 활용\n",
    "\n",
    "정보 이득 = (분할 전 엔트로피) - (분할 후 엔트로피)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└▶ 몸무게 기준 정보 이득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04879494069539847"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - 0.9512050593046015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "└▶ 키 기준 정보 이득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18872187554086717"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - 0.8112781244591328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
